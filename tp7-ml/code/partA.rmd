

```{r}
#usar ml
library(caret)

#para poder usar %>%
library(magrittr) 

#para manejar datasets
library(dplyr)
library(readr)

#Multicore
library(parallel)
cl <- makeCluster(numCores-1)  
parallel::makeCluster(spec = cl)

#Plot
#para gráficos hackers
library(ggdark)
library(ggplot2)

library(tidyverse)

#Algoritmos de ml
library(rpart)
library(rpart.plot)

#Para combinar datasets
library(pandas)

```

# EJERCICIO 1

```{r}
#obtenemos el dataset
trees <- read_csv(file = "../arbolado-publico-mendoza-2021/arbolado-mza-dataset.csv")

#generamos un vector con los indices del conjunto trees
x <- seq(1:nrow(trees))
#Calculamos en 80%
size <- (nrow(trees) * 80) / 100 

#Obtenemos los indices del 80%
idx <- sample(x,size)

#Seleccionamos el conjunto de prueba
validation <- trees[-idx,]
write_csv(x = validation, file = "../data/arbolado-publico-mendoza-2021-validation.csv")

#Seleccionamos el conjunto de entrenamiento
train <- trees[idx,]
write_csv(x = train, file = "../data/arbolado-publico-mendoza-2021-train.csv")

```
# EJERCICIO 2

## A

```{r}
train %>% 
  group_by(inclinacion_peligrosa) %>% 
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=inclinacion_peligrosa, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()
```
## B

```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  group_by(nombre_seccion) %>%
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=nombre_seccion, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1), legend.position = "none")

```
## C
```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  group_by(especie) %>%
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=especie, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1), legend.position = "none")
```

# EJERCICIO 3
## B

```{r}
train %>%
  ggplot()+
  geom_histogram(aes(x=circ_tronco_cm), binwidth = 10,bins = 20, fill = "green", color="black")+
  dark_theme_classic()
```

## C

```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  ggplot()+
  geom_histogram(aes(x=circ_tronco_cm), binwidth = 10,bins = 20, fill = "green", color="black")+
  dark_theme_classic()
```

## D

```{r}
#Obtiene los cuartiles y la mediana para saber a partir de que valor dividir
quantile(train$circ_tronco_cm)

#Agrega la variable
divideTrain <- train %>%
  mutate(
    circ_tronco_cm_cat = 
      ifelse(
        circ_tronco_cm <= 60,'Bajo',
        ifelse(
          60 < circ_tronco_cm & circ_tronco_cm <= 110,'Medio',
          ifelse(
            110 < circ_tronco_cm & circ_tronco_cm <= 158, 'Alto',
            'Muy alto'
          )
        )
      )
  )
write_csv(x = divideTrain, file = "../data/arbolado-publico-mendoza-2021-circ_tronco_cm-train.csv")

```
# EJERCICIO 4

```{r}

# INCISO A

createRandomColumn <- function(dataFrame){
  dataFrame$prediction_prob <- runif(nrow(dataFrame), 0 , 1)
  return(dataFrame)
}

# INCISO B

random_classifier <- function (dataFrame){
  dataFrame <- dataFrame %>% mutate(
    prediction_class = ifelse(
      prediction_prob > 0.5, 1, 0
    )
  ) %>%
  select(-prediction_prob)
  return(dataFrame)
}

# INCISO C

validation <- read_csv(file = "../data/arbolado-publico-mendoza-2021-validation.csv") %>%
  createRandomColumn() %>%
  random_classifier()

# INCISO D

confusionMatrix(
  as.factor(validation$inclinacion_peligrosa), 
  as.factor(validation$prediction_class)
  )

```

# EJERCICIO 5

```{r}

# INCISO A

biggerclass_classifier <- function (dataFrame) {
  #Contamos lacantidad de arboles con inclinacion peligrosa
  a <- dataFrame %>% 
    group_by(inclinacion_peligrosa) %>% 
    summarise(total=n())
  
  #ordenamos por cantidad
  order( a[,"total"], decreasing = TRUE )
  
  #Seleccionamos el valor con mayor cantidad
  b <- a$inclinacion_peligrosa[1]
  
  #actualizamos el dataframe
  dataFrame <- dataFrame %>% mutate(prediction_class = b)
  return(dataFrame)
}

# INCISO B.C

validation <- read_csv(file = "../data/arbolado-publico-mendoza-2021-validation.csv") %>%
  biggerclass_classifier()

# INCISO B.D

validation %>% dataMatrix()


```

# EJERCICIO 6

```{r}
dataMatrix <- function(validation){
  truePositive <- validation %>% filter(inclinacion_peligrosa == 1 & prediction_class == 1)
  trueNegative <- validation %>% filter(inclinacion_peligrosa == 0 & prediction_class == 0)
  falsePositive <- validation %>% filter(inclinacion_peligrosa == 0 & prediction_class == 1)
  falseNegative <- validation %>% filter(inclinacion_peligrosa == 1 & prediction_class == 0)
  rows <- nrow(validation)
  tpCount <- nrow(truePositive)
  tnCount <- nrow(trueNegative)
  fpCount <- nrow(falsePositive)
  fnCount <- nrow(falseNegative)
  titles <- c('True positive', 'True negative', 'False positive', 'False negative', 'Specifity', 'Sensitivity', 'Accuracy', 'Precision')
  values <- c(tpCount, tnCount, fpCount, fnCount,
    (tpCount/(tpCount+fnCount)),
    (tnCount/tnCount+fpCount), 
    (tpCount + tnCount) / rows,
    (tpCount / (tpCount + fpCount))
    )
  return(data.frame(titles, values))
}
```


# EJERCICIO 7

```{r}

# INCISO A

create_folds <- function (dataFrame, k) {
  foldsLenght <- dataFrame %>% nrow() / k %>% ceiling()
  
  value <- split(dataFrame, sample(rep(1:k,foldsLenght)))
  
  return(value)
}

# INCISO B

cross_validation <- function (dataFrame, nFolds){
  #Dividimos el conjunto de entrenamiento
  folds <- create_folds(dataFrame, nFolds)
  
  #Decidiremos si un arbol tiene inclincacion peligrosa a partir de las variables seleccionadas
  formulaTrain <- formula(inclinacion_peligrosa ~ altura+diametro_tronco+especie)
  

  #Valores a retornar
  itera <- NULL
  specifity <- NULL
  sensivity <- NULL
  accuracy <- NULL
  precision_ <- NULL
  media <- NULL
  desviacion_estandar <- NULL
  
  
  #Por cada fold a analizar
  for(i in 1:nFolds){
    #Selecciona el conjunto de entrenamiento y test
    dataTrain <- folds[-n] %>% combine()
    dataTest <- folds[n] %>% toDataFrame()
    #Entrena el modelo
    tree_model<-rpart(formula = formulaTrain, data=dataTrain)
    # obtenemos la predicción
    prediction_class<-predict(tree_model,dataTest,type='class')
    inclinacion_peligrosa <- dataTest$inclinacion_peligrosa
    
    #Calcula los resultados a devolver

    compare <- data.frame(inclinacion_peligrosa, prediction_class)
    results <- dataMatrix(compare)
    
    itera[i] = i
    specifity[i] = results$value[5]
    sensivity[i] = results$value[6]
    accuracy[i] = results$value[7]
    precision_[i] = results$value[8]
    media[i] = "null"
    desviacion_estandar[i] = "null"
  }
  return(data.frame(itera, specifity, sensivity, accuracy, precision_, media, desviacion_estandar))
}


#Combina una lista con dataframes
combine <- function (dataList) {
  dim <- length(dataList)
  initData <- toDataFrame(dataList[1])
  for (v in dataList[2:dim]){
    initData <- rbind(initData, v)
  }
  return(initData)
}
toDataFrame <- function (dataFrame){
  for(v in dataFrame){return(v)}
}

cross_validation(trees, 6)



```
