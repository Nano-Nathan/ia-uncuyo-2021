

```{r}
#usar ml
library(caret)

#para poder usar %>%
library(magrittr) 

#para manejar datasets
library(dplyr)
library(readr)

#Multicore
library(parallel)
cl <- makeCluster(numCores-1)  
parallel::makeCluster(spec = cl)

#Plot
#para gráficos hackers
library(ggdark)
library(ggplot2)

library(tidyverse)

#Algoritmos de ml
library(rpart)

```

# EJERCICIO 1

```{r}
#obtenemos el dataset
trees <- read_csv(file = "../arbolado-publico-mendoza-2021/arbolado-mza-dataset.csv")

#generamos un vector con los indices del conjunto trees
x <- seq(1:nrow(trees))
#Calculamos en 80%
size <- (nrow(trees) * 80) / 100 

#Obtenemos los indices del 80%
idx <- sample(x,size)

#Seleccionamos el conjunto de prueba
validation <- trees[-idx,]
write_csv(x = validation, file = "../data/arbolado-publico-mendoza-2021-validation.csv")

#Seleccionamos el conjunto de entrenamiento
train <- trees[idx,]
write_csv(x = train, file = "../data/arbolado-publico-mendoza-2021-train.csv")

```
# EJERCICIO 2

## A

```{r}
train %>% 
  group_by(inclinacion_peligrosa) %>% 
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=inclinacion_peligrosa, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()
```
## B

```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  group_by(nombre_seccion) %>%
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=nombre_seccion, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1), legend.position = "none")

```
## C
```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  group_by(especie) %>%
  summarise(total=n()) %>%
  ggplot()+
  geom_col(aes(x=especie, y=total), fill='green', color='black')+
  ggdark::dark_theme_classic()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1), legend.position = "none")
```

# EJERCICIO 3
## B

```{r}
train %>%
  ggplot()+
  geom_histogram(aes(x=circ_tronco_cm), binwidth = 10,bins = 20, fill = "green", color="black")+
  dark_theme_classic()
```

## C

```{r}
train %>%
  filter(inclinacion_peligrosa == 1) %>%
  ggplot()+
  geom_histogram(aes(x=circ_tronco_cm), binwidth = 10,bins = 20, fill = "green", color="black")+
  dark_theme_classic()
```

## D

```{r}
#Obtiene los cuartiles y la mediana para saber a partir de que valor dividir
quantile(train$circ_tronco_cm)

#Agrega la variable
divideTrain <- train %>%
  mutate(
    circ_tronco_cm_cat = 
      ifelse(
        circ_tronco_cm <= 60,'Bajo',
        ifelse(
          60 < circ_tronco_cm & circ_tronco_cm <= 110,'Medio',
          ifelse(
            110 < circ_tronco_cm & circ_tronco_cm <= 158, 'Alto',
            'Muy alto'
          )
        )
      )
  )
write_csv(x = divideTrain, file = "../data/arbolado-publico-mendoza-2021-circ_tronco_cm-train.csv")

```
# EJERCICIO 4

```{r}

# INCISO A

createRandomColumn <- function(dataFrame){
  dataFrame$prediction_prob <- runif(nrow(dataFrame), 0 , 1)
  return(dataFrame)
}

# INCISO B

random_classifier <- function (dataFrame){
  dataFrame <- dataFrame %>% mutate(
    prediction_class = ifelse(
      prediction_prob > 0.5, 1, 0
    )
  ) %>%
  select(-prediction_prob)
  return(dataFrame)
}

# INCISO C

validation <- read_csv(file = "../data/arbolado-publico-mendoza-2021-validation.csv") %>%
  createRandomColumn() %>%
  random_classifier()

# INCISO D

confusionMatrix(
  as.factor(validation$inclinacion_peligrosa), 
  as.factor(validation$prediction_class),
  mode = "everything"
  )

```

# EJERCICIO 5

```{r}

# INCISO A

biggerclass_classifier <- function (dataFrame) {
  #Contamos lacantidad de arboles con inclinacion peligrosa
  a <- dataFrame %>% 
    group_by(inclinacion_peligrosa) %>% 
    summarise(total=n())
  
  #ordenamos por cantidad
  order( a[,"total"], decreasing = TRUE )
  
  #Seleccionamos el valor con mayor cantidad
  b <- a$inclinacion_peligrosa[1]
  
  #actualizamos el dataframe
  dataFrame <- mutate(dataFrame, prediction_class = as.integer(b))
  return(dataFrame)
}

# INCISO B.C

dataFrame <- read_csv(file = "../data/arbolado-publico-mendoza-2021-validation.csv")

dataFrame$inclinacion_peligrosa <- as.integer(dataFrame$inclinacion_peligrosa)

validation <- biggerclass_classifier(dataFrame)

validation %>% group_by(inclinacion_peligrosa) %>% summarise(total = n())
validation %>% group_by(prediction_class) %>% summarise(total = n())



# INCISO B.D



inclinacion_peligrosa_h<-factor(validation$inclinacion_peligrosa)

predictions<-factor(validation$prediction_class, levels = levels(inclinacion_peligrosa_h))

confusionMatrix(
  inclinacion_peligrosa_h,
  predictions,
  mode = "everything"
  )

as.factor(validation$prediction_class)
validation %>% dataMatrix()
```

# EJERCICIO 6

```{r}
dataMatrix <- function(validation){
  truePositive <- validation %>% filter(inclinacion_peligrosa == 1 & prediction_class == 1)
  trueNegative <- validation %>% filter(inclinacion_peligrosa == 0 & prediction_class == 0)
  falsePositive <- validation %>% filter(inclinacion_peligrosa == 0 & prediction_class == 1)
  falseNegative <- validation %>% filter(inclinacion_peligrosa == 1 & prediction_class == 0)
  rows <- nrow(validation)
  tpCount <- nrow(truePositive)
  tnCount <- nrow(trueNegative)
  fpCount <- nrow(falsePositive)
  fnCount <- nrow(falseNegative)
  titles <- c('True positive', 'True negative', 'False positive', 'False negative', 'Specifity', 'Sensitivity', 'Accuracy', 'Precision')
  values <- c(tpCount, tnCount, fpCount, fnCount,
    (tpCount/(tpCount+fnCount)),
    (tnCount/tnCount+fpCount), 
    (tpCount + tnCount) / rows,
    (tpCount / (tpCount + fpCount))
    )
  return(data.frame(titles, values))
}
```


# EJERCICIO 7

```{r}

# INCISO A

create_folds <- function (dataFrame, k) {
  foldsLenght <- dataFrame %>% nrow() / k %>% ceiling()
  
  value <- split(dataFrame, sample(rep(1:k,foldsLenght)))
  
  return(value)
}

# INCISO B

cross_validation <- function (dataFrame, nFolds){
  #Dividimos el conjunto de entrenamiento
  folds <- create_folds(dataFrame, nFolds)
  
  #Decidiremos si un arbol tiene inclincacion peligrosa a partir de las variables seleccionadas
  train_formula<- formula(inclinacion_peligrosa ~ especie + altura + diametro_tronco)

  #Valores a retornar
  itera <- NULL
  tp <- NULL
  tn <- NULL
  fp <- NULL
  fn <- NULL
  specifity <- NULL
  sensivity <- NULL
  accuracy <- NULL
  precision_ <- NULL
  
  #Por cada fold a analizar
  for(i in 1:nFolds){
    #Selecciona el conjunto de entrenamiento y test
    dataTrain <- folds[-i] %>% combine()
    dataTest <- folds[i] %>% toDataFrame()
    
    #Entrena el modelo
    data_model <- rpart(train_formula,data=dataTrain, method = "class")

    # obtenemos la predicción
    prediction_class <- predict(data_model, dataTest, type="class")
    
    #Calcula los resultados a devolver
    compare <- data.frame(inclinacion_peligrosa = dataTest$inclinacion_peligrosa, prediction_class)
    results <- dataMatrix(compare)
    
    itera[i] = i
    tp[i] = results$value[1]
    tn[i] = results$value[2]
    fp[i] = results$value[3]
    fn[i] = results$value[4]
    specifity[i] = results$value[5]
    sensivity[i] = results$value[6]
    accuracy[i] = results$value[7]
    precision_[i] = results$value[8]
  }
  return(data.frame(
    itera,
    true_positives = tp,
    true_negatives = tn,
    false_positives = fp,
    false_negatives = fn,
    specifity, 
    sensivity, 
    accuracy, 
    precision = precision_
    ))
}


#Combina una lista con dataframes
combine <- function (dataList) {
  dim <- length(dataList)
  initData <- toDataFrame(dataList[1])
  for (v in dataList[2:dim]){
    initData <- rbind(initData, v)
  }
  return(initData)
}
toDataFrame <- function (dataFrame){
  for(v in dataFrame){return(v)}
}

trees <- read_csv(file = "../arbolado-publico-mendoza-2021/arbolado-mza-dataset.csv") %>%
  select(-c(ultima_modificacion, long, lat, area_seccion, nombre_seccion, circ_tronco_cm, seccion))


delete1 <- trees %>% filter(especie == "rbol del cielo")
delete2 <- trees %>% filter(especie == "Maiten")

trees <- trees[trees$id != delete1$id,]
trees <- trees[trees$id != delete2$id,]

trees %>% group_by(inclinacion_peligrosa) %>% summarise(total = n())

with <- trees %>% filter(inclinacion_peligrosa == 1)
  
with_extra_idx <- sample(1:nrow(with),replace = F, size = 1000)
with_extra <- with[with_extra_idx,]

with <- rbind(with, with_extra)



without <- trees %>% filter(inclinacion_peligrosa == 0)

without_idx <- sample(1:nrow(without),replace = F, size = 4600)
without <- without[without_idx,]


dataFrame <- rbind(with, without)

results <- cross_validation(dataFrame, 10)

write_csv(results, "results.csv")

```
